{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Process Data\n",
    "# 1. Normalize all data using sliding window normalization for each set\n",
    "# 2. Split the data into windows\n",
    "# 3. Extract features from each window\n",
    "# 4. Split the data into train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = [\"..\\data\\LK_pressed_with_states.csv\"]\n",
    "\n",
    "# Getting a pandas with normalized signal and state info!\n",
    "\n",
    "raw_data = pd.read_csv(\"..\\data\\LK_pressed_with_states.csv\", delimiter=',')\n",
    "raw_data = raw_data.dropna()\n",
    "raw_data = raw_data[raw_data.columns[raw_data.columns.isin(['Time', 'Signal', 'State'])]]\n",
    "raw_signal = raw_data['Signal'].to_numpy()\n",
    "\n",
    "# def sliding_window_normalization(signal, L_norm):\n",
    "#     signal = np.array(signal)\n",
    "#     normalized_signal = np.zeros_like(signal)\n",
    "    \n",
    "#     for t in range(len(signal)):\n",
    "#         if t >= L_norm - 1:\n",
    "#             # the sliding window\n",
    "#             window = signal[t - L_norm + 1:t + 1]\n",
    "#             mean = np.mean(window)\n",
    "#             std = np.std(window)\n",
    "            \n",
    "#             # Normalize the current value\n",
    "#             normalized_signal[t] = (signal[t] - mean) / std if std != 0 else 0\n",
    "#         else:\n",
    "#             # For the initial values, normalization is skipped or handled differently\n",
    "#             normalized_signal[t] = None  \n",
    "\n",
    "#     return normalized_signal\n",
    "\n",
    "normalized_signal = (raw_signal - np.mean(raw_signal)) / np.std(raw_signal) #sliding_window_normalization(raw_signal, 250)\n",
    "raw_data[\"Signal\"] = normalized_signal\n",
    "df = raw_data.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into windows and getting features!\n",
    "\n",
    "def percentile20(data):\n",
    "    return np.percentile(data,20)\n",
    "\n",
    "def skew(data):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    N = np.size(data)\n",
    "    top = np.sum((data - mean)**3)\n",
    "    bottom = (N-1)*(std**3)\n",
    "    total = top/(bottom + 0.0000001)\n",
    "    return total\n",
    "\n",
    "def kurtosis(data):\n",
    "    mean = np.mean(data)\n",
    "    top = np.sum((data - mean)**4)\n",
    "    bottom = np.sum((data - (mean)**2)**2)\n",
    "    total = top/(bottom + 0.00000001)\n",
    "    return total\n",
    "\n",
    "def stdfd(data):\n",
    "    difference = np.diff(data)\n",
    "    std = np.std(difference)\n",
    "    return std\n",
    "\n",
    "slice_length = 10\n",
    "\n",
    "mean_list = []\n",
    "max_list = []\n",
    "min_list = []\n",
    "range_list = []\n",
    "last_list = []\n",
    "second_list = []\n",
    "perc_list = []\n",
    "skew_list = []\n",
    "kurt_list = []\n",
    "stdiff_list = []\n",
    "state_list = []\n",
    "\n",
    "# Take slice_length number of signal points\n",
    "for i in range(0, len(df) - slice_length + 1):\n",
    "    current_frame = df.iloc[i: i + slice_length - 1]\n",
    "        \n",
    "    # Standard features\n",
    "    mean_list.append(current_frame[ \"Signal\" ].mean())\n",
    "    max_list.append(current_frame[ \"Signal\" ].max())\n",
    "    min_list.append(current_frame[ \"Signal\" ].min())\n",
    "    range_list.append(current_frame[ \"Signal\" ].max() - current_frame[ \"Signal\" ].min())\n",
    "    \n",
    "    perc_list.append(percentile20(current_frame[ \"Signal\" ]))\n",
    "    skew_list.append(skew(current_frame[ \"Signal\" ]))\n",
    "    kurt_list.append(kurtosis(current_frame[ \"Signal\" ]))\n",
    "    stdiff_list.append(stdfd(current_frame[\"Signal\"]))\n",
    "    \n",
    "    # Other features of interest\n",
    "    last_list.append(current_frame.iloc[-1][\"Signal\"])\n",
    "    second_list.append(current_frame.iloc[-2][\"Signal\"])\n",
    "    \n",
    "    # State, Y\n",
    "    state_list.append(current_frame.iloc[-1][\"State\"])\n",
    "    \n",
    "# Make feature df for training\n",
    "feature_df = pd.DataFrame({\"Mean\": mean_list, \n",
    "                           \"Max\": max_list, \n",
    "                           \"Min\": min_list, \n",
    "                           \"Range\": range_list,\n",
    "                           \"Last Val\": last_list, \n",
    "                           \"2nd Last Val\": second_list, \n",
    "                           \"Percentile\": perc_list,\n",
    "                           \"Skew\": skew_list,\n",
    "                           \"Kurtosis\": kurt_list,\n",
    "                           \"Stdiff\": stdiff_list,\n",
    "                           \"State\": state_list\n",
    "                          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = feature_df.to_numpy()\n",
    "X = dataset[:, :-1]\n",
    "y = dataset[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.96      0.95       237\n",
      "         1.0       0.97      0.96      0.96       331\n",
      "\n",
      "    accuracy                           0.96       568\n",
      "   macro avg       0.96      0.96      0.96       568\n",
      "weighted avg       0.96      0.96      0.96       568\n",
      "\n",
      "[ 1.62966612  0.23891854  0.7910973  -0.55217876  2.0875778   0.233883\n",
      "  0.14496411  0.15553272  0.08675103 -0.75360599]\n",
      "1.9566517716945058\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression model\n",
    "\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#Extract coefficients and intercept\n",
    "coefficients = logistic_model.coef_[0]\n",
    "intercept = logistic_model.intercept_[0]\n",
    "\n",
    "print(coefficients)\n",
    "print(intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
